# -*- coding: utf-8 -*-
"""SM_Dynamical_loss_functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j7H0Vvye80igTGazsokNEj1B12ZjWvNM
"""

# This code is included as supplementary material for the paper:
# "Tilting the playing field: Dynamical loss functions for machine learning"
# We share it in a Colab notebook because it should be straightfoward to run.
# Please, be aware this code is not optimized and its only purpose is to show the phenomenology 
# discussed in the paper. This code reproduces Fig. 4 in our paper.


!pip install -q git+https://www.github.com/google/neural-tangents

import matplotlib
import numpy as np
import matplotlib.pyplot as plt
import sys
import time
from matplotlib import gridspec
import pickle
import sys
from collections import namedtuple
import os

from jax import grad, jit
from jax.tree_util import tree_multimap

import neural_tangents as nt
from neural_tangents import stax

import jax
import jax.numpy as jnp
from jax import random
from jax.config import config
config.update("jax_enable_x64", True)

import numpy.random as npr
import seaborn as sns

# To compute the eigenvalues of the Hessian using the Lanczos algorithm we use
# this code by Justin Gilmer (https://github.com/google/spectral-density)

!git clone https://github.com/google/spectral-density.git

!cp /content/spectral-density/jax/density.py /content/
!cp /content/spectral-density/jax/hessian_computation.py /content/
!cp /content/spectral-density/jax/lanczos.py /content/

import hessian_computation as hessian_computation
import lanczos as lanczos
import density as density_lib

### GLOBAL VARIABLES FOR THE SIMULATION ###

N = 100           #number of points per class
C = 3             #number of classes
NN_width = 100    #width of the NN
learning_rate = 1


## Shorter simulation, only two first cycles (faster to run) ##
# T = 5000                      # time steps for one period
# total_time = 10000            # total simulation time
# last_period_no_osc = 0        # make A=1 for the last oscillation?

## Uncomment the following lines for the same simulation as in Fig. 4 (approx. 1 hour to run) ## 
T = 5000                      # time steps for one period
total_time = 70000            # total simulation time
last_period_no_osc = 1        # make A=1 for the last oscillation?


start_osc_from_the_middle = 0
time_comp_density = 25           # time step to compute Hessian eigenvalues 
order = 10                       # order of the Lanczos method

# Function to plot the results

def plot_all_together_5(density, grids, NTK_EVALS, W, L, L_weighted, A, eigv_list, eigv_plot_times,  limx_2_t = 100000,limx_1 = 8500, limx_2 = 10500):

    # defining some parameters
    markersize_t = 12
    number_eig = 1
    limy_w = 0.25
    limx_1_t = 0
    tick1 = 20
    tick2 = 5
    size_fig = 40
    poslet1 = 30
    poslet2 = 320
    fontsize = 60
    sns.set(context='talk', font_scale=3.5,  color_codes=True, palette='deep', style='ticks', 
            rc={"font.size":fontsize,"axes.titlesize":fontsize,"axes.labelsize":fontsize, 'mathtext.fontset': 'cm', 'xtick.direction': 'in','ytick.direction': 'in', 'axes.linewidth': 5.0})
    xlab = lambda name: plt.xlabel(name, fontsize=fontsize)
    ylab = lambda name: plt.ylabel(name, fontsize=fontsize)


    max_eval = [l[-1] for l in NTK_EVALS]
    times = np.array(eigv_plot_times)
    
    fig5 = plt.figure(figsize=(60,size_fig),constrained_layout=True)
    spec5 = fig5.add_gridspec(ncols=2, nrows=6, width_ratios=[6,1])


    ax = fig5.add_subplot(spec5[0, 0])
    W = np.array(W)
    colors_w = [np.array([176/256, 27/256, 42/256, 1]),'gold', 'darkblue']
    for ccc in range(3): 
        plt.plot(W[:,ccc],'.',c=colors_w[ccc],markersize=markersize_t)
    plt.xlim((limx_1_t,limx_2_t))
    ax.set_xticklabels([])
    plt.tick_params(which = 'both', direction='in', length=tick1, width=tick2)
    ylab('$\Gamma_i$')
    
    
    ax = fig5.add_subplot(spec5[0, 1])
    W = np.array(W)
    colors_w = [np.array([176/256, 27/256, 42/256, 1]),'gold', 'darkblue']
    for ccc in range(3): 
        plt.plot(W[:,ccc],'.',c=colors_w[ccc],markersize=markersize_t)
    plt.xlim((limx_1,limx_2))
    plt.tick_params(which = 'both', direction='in', length=tick1, width=tick2)
    ax.set_xticklabels([])

    ax = fig5.add_subplot(spec5[1, 0])
    plt.semilogy(L_weighted,'.' , markersize=markersize_t)
    plt.xlim((limx_1_t,limx_2_t))
    plt.tick_params(which = 'both', direction='in', length=tick1, width=tick2)
    ax.set_xticklabels([])
    ylab('$\mathcal{F}$')
    
    
    ax = fig5.add_subplot(spec5[1, 1])
    plt.plot(L_weighted,'.' , markersize=markersize_t)
    plt.ylim((0.2,0.6))
    plt.xlim((limx_1,limx_2))
    plt.tick_params(which = 'both', direction='in', length=tick1, width=tick2)
    ax.set_xticklabels([])
    
    
    ax = fig5.add_subplot(spec5[2, 0])
    plt.plot(np.array(L_weighted[:-1]) - np.array(L_weighted[1:]),'.' , markersize=markersize_t)
    plt.ylim((-limy_w,limy_w))
    plt.xlim((limx_1_t,limx_2_t))
    ax.set_xticklabels([])
    plt.tick_params(direction='in', length=tick1, width=tick2)
    ylab('$\mathcal{F}(t) - \mathcal{F}(t - 1)$')
    
    
    ax = fig5.add_subplot(spec5[2, 1])
    plt.plot(np.array(L_weighted[:-1]) - np.array(L_weighted[1:]),'.' , markersize=markersize_t)
    plt.ylim((-0.15,0.15))
    plt.xlim((limx_1,limx_2))
    plt.tick_params(direction='in', length=tick1, width=tick2)
    ax.set_xticklabels([])
  

    ax = fig5.add_subplot(spec5[3, 0])
    plt.plot(A,'.' , markersize=markersize_t)
    plt.xlim((limx_1_t,limx_2_t))
    ax.set_xticklabels([])
    plt.tick_params(direction='in', length=tick1, width=tick2)
    ylab('$Accuracy$') 
    
    ax = fig5.add_subplot(spec5[3, 1])
    plt.plot(A,'.' , markersize=markersize_t)
    plt.ylim((0.2,0.6))
    plt.xlim((limx_1,limx_2))
    ax.set_xticklabels([])
    plt.tick_params(direction='in', length=tick1, width=tick2)
    
    ax = fig5.add_subplot(spec5[4, 0])
    for iii in range(number_eig):
        plt.semilogy(eigv_plot_times,eigv_list[:,iii],'.', markersize=markersize_t)
    plt.xlim((limx_1_t,limx_2_t))
    ax.set_xticklabels([])
    plt.tick_params( direction='in', length=tick1, width=tick2)
    plt.tick_params(which = 'minor', direction='in', length=tick1/2, width=tick2/2)
    ylab('$Hessian \ \\lambda_{max}$')    
    
    ax = fig5.add_subplot(spec5[4, 1])
    for iii in range(number_eig):
        plt.semilogy(eigv_plot_times,eigv_list[:,iii],'.', markersize=markersize_t)
    plt.xlim((limx_1,limx_2))
    ax.set_xticklabels([])
    plt.tick_params(direction='in', length=tick1, width=tick2)  

    ax = fig5.add_subplot(spec5[5, 0])
    plt.plot(max_eval,'.' , markersize=markersize_t)    
    plt.xlim((limx_1_t,limx_2_t))
    plt.tick_params(direction='in', length=tick1, width=tick2)
    ylab('$NTK \ \\lambda_{max}$')
    xlab('$t$')
    

    ax = fig5.add_subplot(spec5[5, 1])
    plt.plot(max_eval,'.' , markersize=markersize_t)    
    plt.xlim((limx_1,limx_2))
    plt.tick_params(direction='in', length=tick1, width=tick2)
    plt.ylim((0,1000))
    xlab('$t$')

    plt.show()

# Creating the dataset
def make_dataset(points_per_class, classes, revolutions=4):
  np.random.seed(0)

  N = points_per_class
  C = classes
  pi = np.pi

  X = np.zeros((N * C, 2))
  y = np.zeros((N * C, C))

  for j in range(C):
    ix = range(N * j, N * (j + 1))
    r = np.linspace(0., 1, N) # radius
    omega = 2 * pi / C
    theta_max = revolutions * pi
    t = np.linspace(omega * j,omega * j + theta_max, N) + np.random.randn(N) * 0.2 # theta
    X[ix] = np.c_[r*np.cos(t), r*np.sin(t)]
    y[ix, j] = 1

  return jax.device_put(X), jax.device_put(y)

X, Y = make_dataset(points_per_class=N, classes=C)

### Defining the Neural Network ###

init_fn, apply_fn, kernel_fn = stax.serial(
                                           stax.Dense(NN_width, parameterization='standard'), 
                                           stax.Relu(), 
                                           stax.Dense(3, parameterization='standard')
                                           )

key = random.PRNGKey(0)

# NTK computation
ntk_fn = jit(nt.empirical_ntk_fn(apply_fn))
@jit
def ntk_evals(params, X):
  ntk = ntk_fn(X, X, params)
  evals, _ = jnp.linalg.eigh(ntk)
  return evals

# Weights for the loss function
def c_fn(t,i,w_max):
  slope = 2 * (w_max - 1) / T 
  w_main_class = jnp.where(t < T / 2., 1+ t * slope, 2 * w_max - t * slope - 1)
  res = jnp.ones(C) +  (w_main_class-1) * jnp.eye(C)[i]
  res = res / jnp.sum(res) * C
  return res

# Dynamical loss function
@jit
def weighted_loss(params, X, Y, t, i,w_max):
  w = c_fn(t,i,w_max)
  return -jnp.mean(jax.nn.log_softmax(apply_fn(params, X)) * Y * w) * C

# Standard loss function
@jit
def loss(params, X, Y):
  return -jnp.mean(jax.nn.log_softmax(apply_fn(params, X)) * Y) * C

# Accuracy
@jit
def accuracy(params, X, Y):
  return jnp.mean(jnp.argmax(apply_fn(params, X), axis=1) == jnp.argmax(Y, axis=1))

# Function that takes one minimization step
@jit
def step_CE(_, t_and_params_and_c):
  t, params, c, w_max = t_and_params_and_c
  g = grad(weighted_loss)(params, X, Y, t, c, w_max)
  return t + 1, tree_multimap(lambda x, dx: x - learning_rate * dx, params, g), c, w_max


### CODE TO COMPUTE THE EIGENVALUES OF THE HESSIAN USING THE LANCZOS ALGORITHM IN JAX ###
### this part of the code has been copied and modified from code by Justin Gilmer (https://github.com/google/spectral-density) ###

num_train = X.shape[0]
num_complete_batches, leftover = divmod(num_train, num_train)
num_batches = num_complete_batches + bool(leftover)

print('num_train,num_complete_batches, leftover,num_batches,bool(leftover)',num_train,num_complete_batches, leftover,num_batches,bool(leftover))

batch_size = np.shape(X)[0]
print('batch_size',batch_size)

def data_stream():
  rng = npr.RandomState(0)
  while True:
    perm = rng.permutation(num_train)
    for i in range(num_batches):
      batch_idx = perm[i * batch_size:(i + 1) * batch_size]
      yield X[batch_idx], Y[batch_idx]

batches = data_stream()

batches_list = [next(batches) for i in range(num_batches)]
def batches_fn():
  for b in batches_list:
    yield b

  
def compute_density_eigenvalues(params,batches_fn,t,class_w,weight,i):
    
  loss_hvp = lambda params, batch: weighted_loss(params, batch[0], batch[1], t, class_w, weight)

  hvp, unravel, num_params = hessian_computation.get_hvp_fn(loss_hvp, params, batches_fn)
  hvp_cl = lambda v: hvp(params, v) / len(batches_list) # Match the API required by lanczos_alg

  rng = random.PRNGKey(0)
  rng, split = random.split(rng)
  tridiag, vecs = lanczos.lanczos_alg(hvp_cl, num_params, order, split)

  eig_vals, all_weights = density_lib.tridiag_to_eigv([tridiag])

  density, grids = density_lib.eigv_to_density(eig_vals, all_weights,
                                  grid_len=10000,
                                  sigma_squared=1e-5)

  return density, grids, eig_vals


#######################
### TRAINING THE NN ###
#######################

w_max_list = [70] # A. It must be >= 1

for w_max in w_max_list:

  ind = 0

  print('\n ', 'NEW SIMULATION A: ', w_max, '\n ')

  L = []
  L_weighted = []
  A = []
  W = []
  NTK_EVALS = []
  eigv_list = []
  eigv_plot_times = []

  if start_osc_from_the_middle:
    t = T // 2 
  else:
    t = 0

  c = 0

  ## NN initialization
  _, params = init_fn(key, X.shape)

  w_max_saved = w_max
  told = time.time()
  
  # Loop of minimization steps
  for i in range(total_time):

    if w_max <1:
      sys.exit('w_max must be >= 1')

    t, params, c, w_max = step_CE(0, (t, params, c, w_max)) 

    if t > T:
      # NEW CYCLE
      tnew = time.time()
      print('Period ',int(i%T) ,' finished. Time steps: ',t,' Computation time: ',tnew-told)
      told = tnew

      t = 0
      c = (c + 1) % 3

      if last_period_no_osc:
        if i>total_time-T:
          print('last period without oscillations')
          w_max = 1

      #Plot
      plot_all_together_5(density, grids, NTK_EVALS, W, L, L_weighted, A, eigv_list_a, eigv_plot_times, limx_2_t = i, limx_1 = 1.6*T, limx_2 = 2.1*T)

    L += [loss(params, X, Y)]
    A += [accuracy(params, X, Y)]
    L_weighted += [weighted_loss(params, X, Y, t, c,w_max)]
    W += [c_fn(t,c,w_max)]
    NTK_EVALS += [ntk_evals(params, X[:50])]

    if i%time_comp_density == 0:
      # Compute eigenvalues of the Hessian
      print('\n time: ', i)
      density, grids, eigvs = compute_density_eigenvalues(params,batches_fn,t,c,w_max, i)
      eigv_list += [[eigvs[0][-1],eigvs[0][-2],eigvs[0][-3]]]
      eigv_list_a = np.array(eigv_list)
      eigv_plot_times += [i]
      ind = ind + 1

  plot_all_together_5(density, grids, NTK_EVALS, W, L, L_weighted, A, eigv_list_a, eigv_plot_times, limx_2_t = total_time, limx_1 = 1.6*T, limx_2 = 2.1*T)